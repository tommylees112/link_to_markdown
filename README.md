# ğŸ”— Link to Markdown

Convert web articles to beautifully formatted Markdown files! Perfect for your Obsidian vault or any markdown-based note-taking system. ğŸ“š

## ğŸŒŸ Features

- ğŸ”„ Convert any web article to clean Markdown
- ğŸ“ Automatically organize files by domain
- ğŸ“ Smart title extraction from content
- ğŸ¨ Clean and consistent formatting
- ğŸ”§ Configurable output directory
- ğŸš€ Async batch processing for better performance
- ğŸ”Œ Compatible with [Instant Data Scraper Chrome extension](https://chromewebstore.google.com/detail/instant-data-scraper/ofaokhiedipichpaobibbnahnkdoiiah)
- ğŸ“Š Smart metadata tracking with domain-specific CSV files
- â¸ï¸ Resume support for interrupted downloads
- ğŸ” Automatic retry handling for rate limits
- ğŸ“ˆ Detailed progress logging and statistics

## ğŸš€ Quick Start

```bash
# Install dependencies
uv sync

# Create command-line alias (add to your ~/.bashrc, ~/.zshrc, or equivalent)
alias l2m='uv run src/main.py'

# Convert a single URL
l2m "https://example.com/article"

# Convert multiple URLs
l2m "https://example.com/article1" "https://example.com/article2"

# Specify output directory
l2m -o ./notes "https://example.com/article"

# Force reprocess existing URLs
l2m --no-skip-existing "https://example.com/article"
```

## ğŸ”Œ Chrome Extension Integration

This tool works seamlessly with the [Instant Data Scraper](https://chrome.google.com/webstore/detail/instant-data-scraper/ofaokhiedipichpaobibbnahnkdoiiah) Chrome extension:

1. Use Instant Data Scraper to collect URLs from any website
2. Export the URLs as CSV
3. Process the CSV directly:

```bash
qsv select -n 1 urls.csv | xargs uv run src/main.py
```

OR

```bash
uv run src/main.py --csv urls.csv --column "relative href"
```

## ğŸ“ Directory Structure

The tool organizes content by domain and tracks metadata:

```
output_dir/
â”œâ”€â”€ example_com/
â”‚   â”œâ”€â”€ meta.csv          # Metadata tracking for this domain
â”‚   â”œâ”€â”€ article1.md       # Article content
â”‚   â””â”€â”€ article2.md       # Article content
â””â”€â”€ another_site/
    â”œâ”€â”€ meta.csv
    â””â”€â”€ article3.md
```

### ğŸ“Š Metadata Tracking

Each domain directory contains a `meta.csv` file that tracks:

- `path`: Relative path to the markdown file
- `title`: Article title
- `url`: Original URL
- `domain`: Source domain
- `status`: Download status (success/failed/rate_limited)
- `error_str`: Error message if failed

Example `meta.csv`:

```csv
path,title,url,domain,status,error_str
article1.md,First Article,https://example.com/1,example_com,success,
article2.md,Second Article,https://example.com/2,example_com,rate_limited,"Rate limited - too many requests"
```

### ğŸ” Processing Flow

1. **URL Filtering**:
   - Checks URLs against domain-specific metadata
   - Skips already processed URLs by default
   - Shows clear summary of skipped/new URLs

2. **Batch Processing**:
   - Processes multiple URLs simultaneously
   - Handles rate limiting gracefully
   - Groups similar errors for better visibility

3. **Progress Tracking**:
   ```
   Processing 15 new URLs out of 20 total
   Skipping 5 already processed URLs
   Processing complete: 12 successful, 2 rate limited, 1 failed
   ```

### ğŸ” Resume and Retry

The tool automatically handles interruptions and rate limits:

- Tracks processed URLs in domain-specific CSV files
- Skips already downloaded content by default
- Records failed downloads with error messages
- Allows forced reprocessing with `--no-skip-existing`

To retry failed downloads:

```bash
# Extract failed URLs from meta.csv
qsv search -s status -v success domain/meta.csv | qsv select url | xargs uv run src/main.py
```

## ğŸ—ï¸ Code Structure

The project is organized into modular components for better maintainability and testing:

```
src/
â”œâ”€â”€ main.py                 # CLI interface and main orchestration
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ document.py        # Document data model
â”‚   â””â”€â”€ metadata.py        # Metadata tracking model
â”œâ”€â”€ metadata/
â”‚   â””â”€â”€ extractor.py       # Metadata management
â”œâ”€â”€ converters/
â”‚   â”œâ”€â”€ html_converter.py  # HTML to Markdown conversion
â”‚   â””â”€â”€ markdown_converter.py  # Markdown file writing
â””â”€â”€ utils/
    â”œâ”€â”€ text_utils.py      # Text processing utilities
    â””â”€â”€ url_utils.py       # URL parsing utilities
```

### ğŸ§© Components

- **Document Model** (`models/document.py`): Represents an article with its content
- **Metadata Model** (`models/metadata.py`): Tracks article processing status
- **Metadata Manager** (`metadata/extractor.py`): Handles CSV operations and URL tracking
- **HTML Converter** (`converters/html_converter.py`): Handles fetching and converting web content
- **Markdown Writer** (`converters/markdown_converter.py`): Manages file organization and writing
- **Utilities** (`utils/`): Common functions for text processing and URL handling

## ğŸŒ Environment Variables

- `OBSIDIAN_PATH`: Default output directory for Markdown files
- `USER_AGENT`: Custom user agent for web requests (optional)

## ğŸ“ License

MIT License - Feel free to use and modify!
